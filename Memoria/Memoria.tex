\documentclass[11pt]{article}

% Paquetes
%===================================================================================================

% Establecemos los márgenes
\usepackage[a4paper, margin=1in]{geometry}

% Separacion entre parrafos
\setlength{\parskip}{1em}

% Paquete para incluir codigo
\usepackage{listings}

% Paquete para incluir imagenes
\usepackage{graphicx}
\graphicspath{ {./Imagenes/} }

% Para fijar las imagenes en la posicion deseada
\usepackage{float}

% Para que el codigo acepte caracteres en utf8
\lstset{literate=
  {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
  {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
  {à}{{\`a}}1 {è}{{\`e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
  {À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
  {ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
  {Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
  {â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
  {Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
  {ã}{{\~a}}1 {ẽ}{{\~e}}1 {ĩ}{{\~i}}1 {õ}{{\~o}}1 {ũ}{{\~u}}1
  {Ã}{{\~A}}1 {Ẽ}{{\~E}}1 {Ĩ}{{\~I}}1 {Õ}{{\~O}}1 {Ũ}{{\~U}}1
  {œ}{{\oe}}1 {Œ}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
  {ű}{{\H{u}}}1 {Ű}{{\H{U}}}1 {ő}{{\H{o}}}1 {Ő}{{\H{O}}}1
  {ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
  {€}{{\euro}}1 {£}{{\pounds}}1 {«}{{\guillemotleft}}1
  {»}{{\guillemotright}}1 {ñ}{{\~n}}1 {Ñ}{{\~N}}1 {¿}{{?`}}1 {¡}{{!`}}1
}

% Para que no se salgan las lineas de codigo
% Para fijar una fuente que resalte
\lstset{breaklines=true, basicstyle=\ttfamily}


% Para que los metadatos que escribe latex esten en español
\usepackage[spanish]{babel}

% Para la bibliografia
% Sin esto, los enlaces de la bibliografia dan un error de compilacion
\usepackage{url}

% Para mostrar graficas de dos imagenes, cada una con su caption, y con un caption comun
\usepackage{subcaption}

% Simbolo de los numeros reales
\usepackage{amssymb}

% Para que los codigos tengan una fuente distinta
\usepackage{courier}

\lstdefinestyle{CustomStyle}{
  language=Python,
  numbers=left,
  stepnumber=1,
  numbersep=10pt,
  tabsize=4,
  showspaces=false,
  showstringspaces=false
  basicstyle=\tiny\ttfamily,
}

% Para incluir tablas en csv
\usepackage{csvsimple}

% Para referenciar secciones usando el nombre de las secciones
\usepackage{nameref}

% Para enumerados dentro de enumerados
\usepackage{enumitem}

% Para mejores tablas
\usepackage{tabularx}

% Para poder tener el mismo identificador en dos tablas separadas
\usepackage{caption}

% Mostrar la página de las referencias en el indice del documento
\usepackage[nottoc,numbib]{tocbibind}

% Para mostrar las matrices
\usepackage{amsmath}

% Metadatos del documento
%===================================================================================================
\title{
    {Aprendizaje Automático - Proyecto Final}\\
    {\emph{Facebook comment volume prediction}}
}

\author{
    {Sergio Quijano Rey - 72103503k}\\
    {sergioquijano@correo.ugr.es} \\
    {} \\
    {Lucía Salamanca López - 77185623s} \\
    {luciasalamanca@correo.ugr.es}\\
    {} \\
    {4º Doble Grado Ingeniería Informática y Matemáticas}
}

\date{\today}

% Separacion entre parrafos
\setlength{\parskip}{1em}

% Contenido del documento
%===================================================================================================
\begin{document}

% Portada del documento
\maketitle
\pagebreak

% Indice de contenidos
\tableofcontents

% Lista de figuras
\listoffigures

% Lista de tablas
\listoftables

\pagebreak

\section{Identificación del problema}

El problema que hemos escogido para esta práctica final consiste en predecir el número de comentarios que un post en \emph{Facebook} va a recibir horas después de ser publicado. La fuente original de los datos se encuentra en \cite{uci:online}.

En dicha página, se encuentra también la referencia al \emph{paper} original \cite{original_paper:paper}. En dicho paper se comenta el proceso de extracción de los datos, que generan los \emph{datasets} con los que hemos trabajado.

\subsection{Problema a resolver}

Estamos ante un problema de regresión, pues nuestro objetivo es predecir el número de comentarios (variable en principio no finita) a partir de unos datos de entrada. Por tanto, tenemos una función objetivo que viene dada por:

$$f: \mathbb{X} \rightarrow \mathbb{Y}$$

donde $\mathbb{X}$ es el conjunto de las 53 variables aleatorias de entrada, e $\mathbb{Y}$ es la variable aleatoria de salida, entera.

Por lo tanto, nuestro objetivo es encontrar tanto una clase de funciones $\mathcal{H}$ como un algorito de aprendizaje $\mathcal{A}$ que nos devuelva un $h \in \mathcal{H}$ que se asemeje a la función objetivo, es decir

$$h(x) \approx f(x), \forall x \in \mathbb{X}$$

La noción de que ambas funciones sean aproximadas se especificará en \emph{\ref{eleccion_modelos}. \nameref{eleccion_modelos}}.

\subsection{Descripción de las características}

Para obtener las características el autor del paper \cite{original_paper:paper} ha empleado un \emph{crawler} programado en \lstinline{JAVA} y el lenguaje de consulta de \emph{Facebook} (\lstinline{FQL}), con el que se ha recogido información de las páginas de \lstinline{Facebook} de más interés \cite{original_paper:paper}. A partir de dicha informacióm, solo se consideran los comentarios que fueron publicados en los últimos tres días respecto al momento en el que el \emph{crawler} recolecta los datos. Esto pues se presupone que los posts más antiguos no van a recibir más atención. También se eliminan los posts de los que faltan comentarios o algún detalle necesario, de ahí que en nuestro dataset no tengamos \emph{missing values}, como se mostrará en \emph{\ref{exploracion_datos}. \nameref{exploracion_datos}}.

Como se ha dicho previamente tenemmos 53 características que podemos dividirlas en distintos subgrupos.

\subsubsection{Características de la página}

Las extracción de características gira entorno a una de las características principales de \emph{Facebook}, las páginas de \emph{Facebook}. Por tanto, se extraerán características referentes a distintas páginas de \emph{Facebook} con la intención de predecir el número de comentarios que obtendrá un nuevo \emph{post} en dichas páginas.

Existen cuatro características referentes a las páginas:

\begin{itemize}
  \item \textbf{Likes de la página}: describe el apoyo de los usuarios hacia ciertos elementos de dicha página, como comentarios, imágenes, estados, posts\ldots

% TODO -- Esto mejorar
  \item \textbf{Categoría de la página}: define el tipo de entidad o persona sobre la que trata la información tratada en dicha página. El dataset contiene un archivo donde se definen todos los tipos posibles de categorías, como por ejemplo: comercio local, marca comercial, producto, artista, entretenimiento, \ldots

  \item \textbf{\emph{Checkin} de la página}: comprobaciones de actos de presencia en un determinado lugar (sólo es válido para páginas institucionales)

  \item \textbf{\emph{Talking About} de la página}: número de usuarios que vuelven a la página tras darle a like a dicha página. Por volver a la página se entienden actividades como comentar, dar like a un post, compartir un post\ldots
\end{itemize}

\subsubsection{Características esenciales}

En este subgrupo se incluye el comportamiento de los comentarios en el post en distintos intervalos de tiempo respecto a distintas métricas. Se dividen en cinco secciones que dependen de una referencia temporal fijada. El autor fija esta referencia en 72h después de la publicación del \emph{post}.

% TODO -- si da tiempo, hacerlo en una tabla que queda mas profesional
\begin{itemize}
  \item \textbf{C1}: número total de comentarios durante las 72 horas previas a la referencia

  \item \textbf{C2}: número total de comentarios durante las  24 horas previas a la referencia

  \item \textbf{C3}: número total de comentarios entre las 48 y 24 horas previas a la referencia

  \item \textbf{C4}: número total de comentarios en las 24h posteriores a publicar la referencia

  \item \textbf{C5}: diferencia de número total de comentarios entre C2 y C3.

\end{itemize}

A partir de estas características base, calculamos las siguientes estadísticas respecto a los post de una misma página:  mínimo, máximo, media, mediana y desviación típica. Con ello se obtienen 25 características adicionales, que, junto con las cinco características base, conforman 30 características de este subgrupo.

\subsubsection{Características relativas al día de la semana}

Para representar el día de la semana en el que el post fue publicado y el día con respecto a la referencia temporal se usan indicadores binarios. Existen 14 categorías de este tipo.

Se usan 14 características binarias. 7 de ellas para indicar el día de la semana en la que se publicó el post (todas cero salvo el día de publicación). Las otras 7 características indican el día de la semana de la referencia fijada (de nuevo, 72h después de la publicación del post).

\subsubsection{Otras características básicas}

Incluye 5 características tipo medatadatos del post, como puede ser la longitud del post o el número de veces que se ha compartido.

\subsection{Exploración del \emph{Dataset}} \label{exploracion_datos}

Con la función \lstinline{explore_dataset}, mostramos algunas estadísticas de las variables aleatorias que componen las columnas de nuestro conjunto de datos. Notar que en esta exploración ya hemos separado el conjunto de \emph{training} y \emph{testing}, con la función \lstinline{split_train_test} que más adelante, en \emph{\ref{preprocesado}. \nameref{preprocesado}}.

La descripción de las variables se muestra en la siguiente tabla:

\begin{table}[H]
  \centering
  \resizebox{0.8\columnwidth}{!}{%
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
    \hline
    \textbf{col}  &         \textbf{mean}&         \textbf{median}&           \textbf{std}&     \textbf{min}&           \textbf{max}&           \textbf{p25}&           \textbf{p75} \\
    \hline
    0    & 1.32e+6&  292911.00&  7.401e+6&    36.0&  4.86e+8&  37149.00&  1.20e+6 \\
    1    & 4.63e+3&       0.00&  2.045e+4&     0.0&  1.86e+5&      0.00&  9.90e+1 \\
    2    & 4.54e+4&    7237.00&  1.237e+5&     0.0&  6.78e+6&    698.00&  5.14e+4 \\
    3    & 2.42e+1&      18.00&  2.001e+1&     1.0&  1.06e+2&      9.00&  3.20e+1 \\
    4    & 1.46e+0&       0.00&  1.872e+1&     0.0&  2.34e+3&      0.00&  0.00e+0 \\
    5    & 4.42e+2&     235.00&  4.958e+2&     0.0&  2.77e+3&     45.00&  7.17e+2 \\
    6    & 5.53e+1&      23.37&  8.565e+1&     0.0&  2.34e+3&      5.51&  7.18e+1 \\
    7    & 3.53e+1&      12.00&  6.842e+1&     0.0&  2.34e+3&      2.00&  4.20e+1 \\
    8    & 6.72e+1&      35.06&  8.117e+1&     0.0&  1.00e+3&      7.88&  1.02e+2 \\
    9    & 1.62e-1&       0.00&  3.337e+0&     0.0&  3.81e+2&      0.00&  0.00e+0 \\
    10   & 2.84e+2&     118.00&  3.758e+2&     0.0&  2.77e+3&     26.00&  4.01e+2 \\
    11   & 2.21e+1&       8.43&  3.599e+1&     0.0&  9.99e+2&      1.91&  2.90e+1 \\
    12   & 7.42e+0&       2.00&  1.980e+1&     0.0&  7.97e+2&      0.00&  8.00e+0 \\
    13   & 4.04e+1&      17.38&  5.450e+1&     0.0&  8.70e+2&      4.10&  6.07e+1 \\
    14   & 2.90e-2&       0.00&  2.209e+0&     0.0&  3.24e+2&      0.00&  0.00e+0 \\
    15   & 2.67e+2&     116.00&  3.257e+2&     0.0&  1.87e+3&     26.00&  3.81e+2 \\
    16   & 1.95e+1&       8.58&  3.062e+1&     0.0&  4.37e+2&      2.03&  2.48e+1 \\
    17   & 4.87e+0&       1.00&  1.311e+1&     0.0&  4.33e+2&      0.00&  5.00e+0 \\
    18   & 3.85e+1&      18.63&  5.041e+1&     0.0&  5.80e+2&      4.09&  5.38e+1 \\
    19   & 1.38e+0&       0.00&  1.642e+1&     0.0&  1.89e+3&      0.00&  0.00e+0 \\
    20   & 4.14e+2&     224.00&  4.719e+2&     0.0&  2.77e+3&     41.00&  6.70e+2 \\
    21   & 5.23e+1&      21.85&  8.017e+1&     0.0&  1.89e+3&      5.21&  6.83e+1 \\
    22   & 3.37e+1&      12.00&  6.459e+1&     0.0&  1.89e+3&      2.00&  4.00e+1 \\
    23   & 6.29e+1&      32.36&  7.611e+1&     0.0&  8.52e+2&      7.60&  9.62e+1 \\
    24   &-2.19e+2&     -92.00&  2.801e+2& -1677.0&  3.81e+2&   -310.00& -2.10e+1 \\
    25   & 2.75e+2&     109.00&  3.730e+2&  -204.0&  2.77e+3&     23.00&  3.79e+2 \\
    26   & 2.58e+0&       0.27&  1.563e+1&  -210.5&  6.39e+2&     -0.48&  2.97e+0 \\
    27   &-1.99e+0&       0.00&  1.237e+1&  -288.0&  7.97e+2&     -2.00&  0.00e+0 \\
    28   & 5.56e+1&      25.54&  7.372e+1&     0.0&  1.33e+3&      5.99&  8.12e+1 \\
    29   & 5.50e+1&      11.00&  1.345e+2&     0.0&  2.34e+3&      2.00&  4.60e+1 \\
    30   & 2.20e+1&       2.00&  7.666e+1&     0.0&  2.09e+3&      0.00&  1.20e+1 \\
    31   & 1.94e+1&       0.00&  6.916e+1&     0.0&  1.59e+3&      0.00&  9.00e+0 \\
    32   & 5.20e+1&      10.00&  1.258e+2&     0.0&  2.18e+3&      2.00&  4.40e+1 \\
    33   & 2.64e+0&       0.00&  9.292e+1& -1277.0&  2.09e+3&     -6.00&  3.00e+0 \\
    34   & 3.51e+1&      35.00&  2.099e+1&     0.0&  7.20e+1&     17.00&  5.30e+1 \\
    35   & 1.64e+2&      97.00&  3.826e+2&     0.0&  2.14e+4&     38.00&  1.72e+2 \\
    36   & 1.17e+2&      13.00&  1.023e+3&     1.0&  1.44e+5&      2.00&  6.00e+1 \\
    37   & 0.00e+0&       0.00&  0.000e+0&     0.0&  0.00e+0&      0.00&  0.00e+0 \\
    38   & 2.37e+1&      24.00&  2.008e+0&     1.0&  2.40e+1&     24.00&  2.40e+1 \\
    39   & 1.22e-1&       0.00&  3.273e-1&     0.0&  1.00e+0&      0.00&  0.00e+0 \\
    40   & 1.43e-1&       0.00&  3.506e-1&     0.0&  1.00e+0&      0.00&  0.00e+0 \\
    41   & 1.49e-1&       0.00&  3.566e-1&     0.0&  1.00e+0&      0.00&  0.00e+0 \\
    42   & 1.56e-1&       0.00&  3.622e-1&     0.0&  1.00e+0&      0.00&  0.00e+0 \\
    43   & 1.45e-1&       0.00&  3.525e-1&     0.0&  1.00e+0&      0.00&  0.00e+0 \\
    44   & 1.45e-1&       0.00&  3.523e-1&     0.0&  1.00e+0&      0.00&  0.00e+0 \\
    45   & 1.37e-1&       0.00&  3.443e-1&     0.0&  1.00e+0&      0.00&  0.00e+0 \\
    46   & 1.43e-1&       0.00&  3.502e-1&     0.0&  1.00e+0&      0.00&  0.00e+0 \\
    47   & 1.31e-1&       0.00&  3.382e-1&     0.0&  1.00e+0&      0.00&  0.00e+0 \\
    48   & 1.37e-1&       0.00&  3.443e-1&     0.0&  1.00e+0&      0.00&  0.00e+0 \\
    49   & 1.50e-1&       0.00&  3.572e-1&     0.0&  1.00e+0&      0.00&  0.00e+0 \\
    50   & 1.49e-1&       0.00&  3.564e-1&     0.0&  1.00e+0&      0.00&  0.00e+0 \\
    51   & 1.44e-1&       0.00&  3.518e-1&     0.0&  1.00e+0&      0.00&  0.00e+0 \\
    52   & 1.44e-1&       0.00&  3.514e-1&     0.0&  1.00e+0&      0.00&  0.00e+0 \\
    53   & 7.43e+0&       0.00&  3.588e+1&     0.0&  1.30e+3&      0.00&  3.00e+0 \\
    \hline
  \end{tabular}
}
\caption{Exploración estadística de los atributos del conjunto de entrenamiento}
\end{table}

En la tabla no hemos mostrado la estadística de \emph{missing values}, pues como ya hemos comentado, no tenemos datos faltantes en este \emph{dataset}. Tampoco mostramos la columna en la que se muestra el tipo, pues todas las variables son o bien flotantes o bien enteras, y por tanto no es necesaria una técnica de codificación de variables categóricas como puede ser \emph{one hot encoding}.

En esta tabla queda clara la necesitada de realizar un proceso de estandarización o bien de normalización. Los modelos que vamos a emplear son muy sensibles a variables en escalas diferentes. Otros directamente se comportam mucho mejor cuando tenemos media cero y desviación típica uno.

Estas escalas diferentes se pueden ver, por ejemplo, con la columna 0 que se mueve en el rango $[36.0, 4.86 * 10^8]$ y que tiene una desviación típica de $7.4 * 10^6$, mientras que la columna 43 se mueve en el rango $[0, 1]$ y tiene una desviación típica de $3.5 * 10$.

\pagebreak

\section{Preprocesamiento de los datos} \label{preprocesado}

El preprocesamiento de los datos se divide en tres etapas: estandarización, selección de componentes principales y transformación polinómica de los datos.

A la hora de aplicar \emph{Cross Validation} para seleccionar nuestro modelo final, compararemos los resultados obtenidos tanto en el conjunto transformado con \emph{PCA} y trasnformaciones polinómicas como en el conjunto de datos original al que solo hemos aplicado estandarización, quedándonos con el conjunto de datos en el que se haya encontrado la hipótesis con mejores resultados.

\subsection{Eliminación de outliers}

A la hora de borrar \emph{outliers} no hemos seguido la técnica básica de eliminar aquellos ejemplos en los que, en alguna variable aleatoria, se alejen de la media más de tres veces la desviación típica. Esta aproximación univariante eliminaba un porcentaje demasiado alto de los datos, y por tanto, hemos decidido emplear una técnica más inteligente a la hora de borrar los valores atípicos, basada en la distancia de Mahalanobis. Esta distancia se parece a la distancia euclíde, sin embargo, se tiene en cuenta la correlación entre las variables aleatorias \cite{wiki_mahalanobis:online}.

Para eliminar los outliers hacemos uso de la función \lstinline{remove_outliers} que a su vez, hace uso de la función proporcionada por \emph{sklearn} \lstinline{EllipticEnvelope}. En esta técnica estamos suponiendo que nuestro \emph{dataset} sigue aproximadamente una distribución Gaussiana multidimensional. Intenta ajustar una elipse a los puntos centrales, ignorando los puntos que quedan fuera de esta elipse ajustada. La distancia de Mahalanobis se usa como una medida de cómo de atípico es un punto de nuestro \emph{dataset} \cite{sklearn_outlier:online}. Esta distancia viene dada por.

$$d_m(x, y) := \sqrt{(x - y)^T \Sigma^{-1} (x - y)}$$

Es importante comentar el uso del parámetro \lstinline{contamination}. Este parámetro indica la proporción de \emph{outliers} que estimamos que tiene nuestro \emph{dataset}, es decir, la \emph{"contaminación"} esperada del \emph{dataset}. En nuestro caso hemos fijado este valor a un $5\%$.

Es importante realizar como primer paso el borrado de \emph{outliers}, pues las posteriores técnicas como estandarización o análisis de componentes principales son muy sensibles a estos \emph{outliers}.

\subsection{Estandarización de los datos}

A la hora de estandarizar los datos, buscamos que las variables aleatorias de nuestro \emph{dataset} sigan una distribución normal de media cero y desviación típica uno.

Para ello, y supuesta la hipótesis de que nuestras variables siguen una distribución normal de media y varianza desconocida, usamos el hecho de que cualquier variable aleatoria $X$ que siga una normal verifica que:
$$X_{norm} := \frac{{X- \bar{X}}}{X_{\sigma}} \Longrightarrow X_{norm} \sim \mathcal{N}(0, 1)$$

La media y la desviación típica son muy sensibles a valores atípicos, por lo que es necesario quitar los \emph{outliers} antes de estandarizar nuestros datos, como ya hemos comentado.

Tras dicha limpieza de outliers aplicamos el estandarizado con la función \lstinline{standarize_dataset}, que hace uso de la función \lstinline{StandardScaler} proporcionada por \emph{sklearn}.

A esta función le pasamos como parámetros tanto el conjunto de entrenamiento como el conjunto de test. El conjunto de test no se usa a la hora de calcular la transformación, solo se emplea el conjunto de entrenamiento para estos cálculos. El conjunto de test se pasa con el único propósito de aplicar la misma trasnformación que la calculada y aplicada en el conjunto de test. Todo esto para no caer en un caso de \emph{data snooping}.

Como hemos comentado previamente, todos los datos con los que vamos a entrenar los distintos modelos van a pasar por estas dos primeras etapas, por lo que, tras haber eliminado los \emph{outliers} y haber estandarizado nuestros datos, haremos una copia de nuestros datos en \lstinline{df_train_original_x} y \lstinline{df_test_original_x}.

Tras estandarizar los datos, mostramos las estadísticas de algunas columnas del conjunto estandarizado:

\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
  \hline
\textbf{col}  &         \textbf{mean}&         \textbf{median}&           \textbf{std}&     \textbf{min}&           \textbf{max}&           \textbf{p25}&           \textbf{p75} \\
\hline
0  &  3.250e-17& -0.34&  1.00&  -0.47&   15.65& -0.45& -0.00\\
1  &  7.671e-16& -0.22&  1.00&  -0.22&    9.02& -0.22& -0.22\\
2  & -1.947e-16& -0.46&  1.00&  -0.56&    9.33& -0.55&  0.17\\
3  &  1.025e-15& -0.32&  1.00&  -1.17&    4.02& -0.77&  0.36\\
\ldots  &  \ldots & \ldots &  \ldots &  \ldots &    \ldots & \ldots &  \ldots \\
50 &  1.250e-16& -0.41&  1.00&  -0.41&    2.40& -0.41& -0.41\\
51 &  4.213e-16& -0.41&  1.00&  -0.41&    2.43& -0.41& -0.41\\
52 & -7.532e-16& -0.41&  1.00&  -0.41&    2.43& -0.41& -0.41\\
\hline
  \end{tabular}
  \caption{Estadísticas del \emph{dataset} tras estandarizar}
  \label{tabla_std}
\end{table}

Nuestro proceso de estandarización en principio solo se preocupa de conseguir media cero y desviación típica uno. Pero como se puede apreciar en la anterior tabla, este proceso provoca colateralmente que el rango de las distintas variables no sea tan dispar, lo cual es beneficioso como ya se ha comentado.

\subsection{Análisis de Componentes Principales (PCA)}

Para el análisis de componentes principales hacemos uso de la función \lstinline{apply_PCA}. Queremos que tras la transformación de nuestro espacio obtengamos el $99\%$ de la varianza de nuestro \emph{dataset} original. Por tanto, pasaremos a la función \lstinline{PCA} de \emph{sklearn} el parámetro \lstinline{n_components} igualado a $0.99$, lo que fuerza a que seleccione el número de componentes principales necesarias para explicar el $99\%$ o más de nuestra varianza.

% TODO -- explica mejor esto chiquilla
Vamos a observar en la siguiente tabla el número de componentes que obtenemos con el $99\%$ de la varianza y la varianza explicada en cada una de ellas. El método de \emph{PCA} devuelve las componentes ordenadas de mayor varianza explicada a menor.

% TODO -- poner solo 3 cifras decimales que Nicolas lo penaliza
\begin{table}[!htb]
  \begin{tabular}{|c|c|}
    \hline
    \textbf{Nº de componente}  &         \textbf{Varianza explicada} \\
    \hline
    1& 0.365 \\
    2& 0.044\\
    3& 0.040\\
    4& 0.039\\
    5& 0.038\\
    6& 0.037\\
    7& 0.032\\
    8& 0.026\\
    9& 0.025 \\
    10& 0.024\\
    11& 0.023\\
    12& 0.021\\
    13& 0.021\\
    14& 0.020\\
    15& 0.020\\
    16& 0.020\\
    \hline
  \end{tabular}
  \quad
  \begin{tabular}{|c|c|}
    \hline
    \textbf{Nº de componente}  &         \textbf{Varianza explicada} \\
    \hline
    17& 0.019\\
    18& 0.019\\
    19& 0.018\\
    20& 0.017\\
    21& 0.016\\
    22& 0.016\\
    23& 0.016\\
    24& 0.014\\
    25& 0.012\\
    26& 0.007\\
    27& 0.007\\
    28& 0.006\\
    29& 0.006\\
    30& 0.005\\
    31& 0.003\\
    \hline
  \end{tabular}
\caption{Varianza explicada por las componentes principales obtenidas}
\end{table}

Tenemos un total de 31 componentes que explican el 0.9906 de la varianza. Obtenemos así que la dimensionalidad de nuestro espacio muestral se reduce un $40\%$ aproximadamente.

Tras aplicar \emph{PCA}, las variables vuelven a estar en rangos dispares. Sin embargo, esperamos a aplicar las trasnformaciones no lineales de los datos para volver a realizar la estandarización. 

\subsection{Transformación polinómica}

% TODO -- hablar de d_vc

Al haber reducido la dimensionalidad de nuestro espacio, el tamaño de nuestro \emph{dataset} permite realizar una transformación cuadrática de nuestros datos sin que el coste computacional sea excesivo, y sin que tengamos problemas de generalización. 

Hemos considerado esta transformación pues no disponemos de conocimiento experto del problema para decidir en base a ello una tal transformación. Tampoco en el \emph{paper} original \cite{original_paper:paper} se presenta una transformación en base a conocimiento experto.

Por tanto, consideramos una transformación polinómica $\phi_q$ de grado $q$ fijado, pues pueden dar buenos resultados sin mucha información a priori. Fijamos $q=2$ por el coste computacional ya mencionado y por buscar una buena generalización. Una transformación polinómica de mayor grado no sería abordable en nuestras máquinas (hicimos la prueba y comprobamos que, efectivamente, no era abordable en un tiempo razonable) y podría llegar a dar problemas de generalización.

% TODO - la frase de los monomios es que la dices mejor y me he enterado regular
La transformación consiste en considerar los monomios en varias variables de hasta grado 2. Por ejemplo, $x_1 x_2, x_1, x_1^2, \ldots$

La dimensión de nuestro espacio transformado es entonces 

$$d = {1+2n}+{{n(n-1)}\over{2}} = {1+2*31}+{{31*30}\over{2}} = 528$$

donde $n$ es el tamaño de nuestro conjunto de entrenamiento.

Ahora, tenemos varianzas y rangos de las variables dispares. Esto es claro porque estamos considerando transformaciones no lineales que no van a respetar las escalas similares de nuestros datos, ni la distribución centrada en cero. Por tanto es necesario aplicar de nuevo estandarización al nuevo conjunto de entrenamiento. El efecto es el mismo que el comentado en la \emph{Tabla \ref{tabla_std}. \nameref{tabla_std}}, y por tanto no mostramos esta tabla ya que no va a aportar información nueva.

\pagebreak

\section{Selección del modelo} \label{seleccion_modelos}

\pagebreak

\
% Bibliografia
\bibliography{./References}
\bibliographystyle{ieeetr}

\end{document}
