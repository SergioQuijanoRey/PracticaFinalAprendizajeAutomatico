Reloaded modules: core
==> Estableciendo semilla inicial
==> Cargamos todos los archivos en un unico dataframe
==> Separando en train y test
--> Nº datos entrenamiento: 32839
--> Nº datos test: 8210
==> Exploramos el conjunto de entrenamiento
Estadisticas del dataset de entrenamiento:
================================================================================
       type          mean  ...           p75  missing vals
0     int64  1.325760e+06  ...  1.204214e+06             0
1     int64  4.639544e+03  ...  9.900000e+01             0
2     int64  4.540827e+04  ...  5.141100e+04             0
3     int64  2.424660e+01  ...  3.200000e+01             0
4   float64  1.463230e+00  ...  0.000000e+00             0
5   float64  4.421631e+02  ...  7.170000e+02             0
6   float64  5.538075e+01  ...  7.182883e+01             0
7   float64  3.532981e+01  ...  4.200000e+01             0
8   float64  6.721688e+01  ...  1.025550e+02             0
9   float64  1.623679e-01  ...  0.000000e+00             0
10  float64  2.849974e+02  ...  4.010000e+02             0
11  float64  2.210564e+01  ...  2.900552e+01             0
12  float64  7.425911e+00  ...  8.000000e+00             0
13  float64  4.045426e+01  ...  6.076033e+01             0
14  float64  2.905082e-02  ...  0.000000e+00             0
15  float64  2.673669e+02  ...  3.810000e+02             0
16  float64  1.951925e+01  ...  2.484252e+01             0
17  float64  4.874859e+00  ...  5.000000e+00             0
18  float64  3.852915e+01  ...  5.385987e+01             0
19  float64  1.384025e+00  ...  0.000000e+00             0
20  float64  4.145740e+02  ...  6.700000e+02             0
21  float64  5.233360e+01  ...  6.834286e+01             0
22  float64  3.373236e+01  ...  4.000000e+01             0
23  float64  6.297207e+01  ...  9.626692e+01             0
24  float64 -2.191868e+02  ... -2.100000e+01             0
25  float64  2.752198e+02  ...  3.790000e+02             0
26  float64  2.586392e+00  ...  2.976744e+00             0
27  float64 -1.993285e+00  ...  0.000000e+00             0
28  float64  5.569373e+01  ...  8.120929e+01             0
29    int64  5.505490e+01  ...  4.600000e+01             0
30    int64  2.206501e+01  ...  1.200000e+01             0
31    int64  1.941551e+01  ...  9.000000e+00             0
32    int64  5.204218e+01  ...  4.400000e+01             0
33    int64  2.649502e+00  ...  3.000000e+00             0
34    int64  3.518947e+01  ...  5.300000e+01             0
35    int64  1.640335e+02  ...  1.720000e+02             0
36    int64  1.177135e+02  ...  6.000000e+01             0
37    int64  0.000000e+00  ...  0.000000e+00             0
38    int64  2.374893e+01  ...  2.400000e+01             0
39    int64  1.225981e-01  ...  0.000000e+00             0
40    int64  1.437315e-01  ...  0.000000e+00             0
41    int64  1.490910e-01  ...  0.000000e+00             0
42    int64  1.560035e-01  ...  0.000000e+00             0
43    int64  1.454064e-01  ...  0.000000e+00             0
44    int64  1.454977e-01  ...  0.000000e+00             0
45    int64  1.376717e-01  ...  0.000000e+00             0
46    int64  1.433052e-01  ...  0.000000e+00             0
47    int64  1.319772e-01  ...  0.000000e+00             0
48    int64  1.372149e-01  ...  0.000000e+00             0
49    int64  1.500350e-01  ...  0.000000e+00             0
50    int64  1.493651e-01  ...  0.000000e+00             0
51    int64  1.440056e-01  ...  0.000000e+00             0
52    int64  1.440970e-01  ...  0.000000e+00             0
53    int64  7.438686e+00  ...  3.000000e+00             0

[54 rows x 9 columns]
==> Borrando outliers
/home/luciasalamanca/anaconda3/lib/python3.8/site-packages/sklearn/covariance/_robust_covariance.py:647: UserWarning: The covariance matrix associated to your dataset is not full rank
  warnings.warn("The covariance matrix associated to your dataset "
Tamaño tras la limpieza de outliers del train_set: 31197
Shapes de X e Y: (31197, 53), (31197,)
Numero de filas eliminadas: 1642
Porcentaje de filas eliminadas: 5.000152257985931%
==> Estandarizando el dataset
==> Aplicando PCA
Ajustando los datos de entrenamiento a la transformacion
Ajuste realizado:
	Porcentaje de la varianza explicado: [0.36572135 0.04396426 0.04024351 0.03983162 0.03807559 0.03742947
 0.0328059  0.02605652 0.02503619 0.02425605 0.02397805 0.02164596
 0.02108977 0.02071851 0.0202135  0.02006357 0.01982559 0.01957728
 0.01890433 0.01723848 0.01697129 0.01676429 0.01634049 0.01433439
 0.01248459 0.00763479 0.00751526 0.00694255 0.00652505 0.00508726
 0.00341686]
	Porcentaje de la varianza explicado total: 0.9906922920022432
	Numero de dimensiones obtenidas: 31
==> Aplicando polinomio grado 2 al conjunto PCA
==> Lanzando cross validation
--> CV -- PCA + Polinimio orden 2
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
[Parallel(n_jobs=-1)]: Done  12 out of  20 | elapsed:   25.3s remaining:   16.9s
[CV] alpha=1 .........................................................
[CV] .......................... alpha=1, score=-244.848, total=  14.1s
[CV] alpha=1.0 .......................................................
[CV] ........................ alpha=1.0, score=-420.592, total=   3.9s
[CV] alpha=1.0 .......................................................
[CV] ........................ alpha=1.0, score=-885.952, total=   3.1s
[CV] alpha=2.0 .......................................................
[CV] ........................ alpha=2.0, score=-259.621, total=   1.9s
[CV] alpha=2.0 .......................................................
[CV] ........................ alpha=2.0, score=-463.994, total=   1.8s
[CV] alpha=1 .........................................................
[CV] .......................... alpha=1, score=-420.592, total=   4.1s
[CV] alpha=1 .........................................................
[CV] .......................... alpha=1, score=-260.741, total=   4.5s
[CV] alpha=1.0 .......................................................
[CV] ........................ alpha=1.0, score=-378.863, total=   7.9s
[CV] alpha=1.0 .......................................................
[CV] ........................ alpha=1.0, score=-260.741, total=   4.5s
[CV] alpha=2.0 .......................................................
[CV] ........................ alpha=2.0, score=-397.410, total=   2.2s
[CV] alpha=2.0 .......................................................
[CV] ........................ alpha=2.0, score=-281.018, total=   1.8s
[CV] alpha=1 .........................................................
[CV] .......................... alpha=1, score=-378.863, total=   8.0s
[CV] alpha=1 .........................................................
[CV] .......................... alpha=1, score=-885.952, total=   3.0s
[CV] alpha=1.0 .......................................................
[CV] ........................ alpha=1.0, score=-244.848, total=  13.0s
[CV] alpha=2.0 .......................................................
[CV] ........................ alpha=2.0, score=-909.079, total=   1.5s
[CV] alpha=0.1 .......................................................
[CV] ........................ alpha=0.1, score=-860.986, total=  30.1s
Resultados para Lasso
================================================================================

	--> Param: {'alpha': 0.1}, mean_score: -412.72791199014773, std_score: 229.15719404601015
	--> Param: {'alpha': 1}, mean_score: -438.1992705047148, std_score: 233.74774127379862
	--> Param: {'alpha': 1.0}, mean_score: -438.1992705047148, std_score: 233.74774127379862
	--> Param: {'alpha': 2.0}, mean_score: -462.224589936069, std_score: 235.69765814381586

Fitting 5 folds for each of 4 candidates, totalling 20 fits
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  6.4min finished
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
[Parallel(n_jobs=-1)]: Done  12 out of  20 | elapsed:    2.4s remaining:    1.6s
Resultados para Ridge
================================================================================

	--> Param: {'alpha': 0.1}, mean_score: -620594.3545732987, std_score: 1239111.7369724675
	--> Param: {'alpha': 1}, mean_score: -48987.51937109696, std_score: 95427.87780524314
	--> Param: {'alpha': 1.0}, mean_score: -48987.51937109696, std_score: 95427.87780524314
	--> Param: {'alpha': 2.0}, mean_score: -22378.935838535876, std_score: 42034.678954169925

Fitting 5 folds for each of 9 candidates, totalling 45 fits
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    2.9s finished
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:  1.4min
[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:  3.4min finished
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
Resultados para MLP
================================================================================

	--> Param: {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (50,)}, mean_score: -394.0110152737194, std_score: 146.64960923991472
	--> Param: {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (75,)}, mean_score: -400.077830243028, std_score: 124.63473067554776
	--> Param: {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (100,)}, mean_score: -441.6521673279366, std_score: 147.93803598318797
	--> Param: {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (50,)}, mean_score: -383.12974289249115, std_score: 128.84893402561676
	--> Param: {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (75,)}, mean_score: -368.49082535369655, std_score: 123.99069773782372
	--> Param: {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (100,)}, mean_score: -372.994003541196, std_score: 140.07155391201437
	--> Param: {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (50,)}, mean_score: -375.3951955882808, std_score: 116.16391937284932
	--> Param: {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (75,)}, mean_score: -413.99379869398473, std_score: 91.17631621349469
	--> Param: {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (100,)}, mean_score: -415.06474507792575, std_score: 138.91850785335185

Fitting 5 folds for each of 3 candidates, totalling 15 fits
[Parallel(n_jobs=-1)]: Done   6 out of  15 | elapsed: 96.5min remaining: 144.7min
[CV] alpha=0.1 .......................................................
[CV] ........................ alpha=0.1, score=-359.470, total= 6.4min
[CV] alpha=0.1 .......................................................
[CV] ....................... alpha=0.1, score=-3079.525, total=   0.8s
[CV] alpha=1.0 .......................................................
[CV] ..................... alpha=1.0, score=-239820.397, total=   0.8s
[CV] alpha=2.0 .......................................................
[CV] ........................ alpha=2.0, score=-326.400, total=   0.8s
[CV] activation=relu, alpha=0.001, hidden_layer_sizes=(50,) ..........
[CV]  activation=relu, alpha=0.001, hidden_layer_sizes=(50,), score=-368.681, total=  17.1s
[CV] activation=relu, alpha=0.001, hidden_layer_sizes=(75,) ..........
[CV]  activation=relu, alpha=0.001, hidden_layer_sizes=(75,), score=-580.757, total=  40.4s
[CV] activation=relu, alpha=0.01, hidden_layer_sizes=(50,) ...........
[CV]  activation=relu, alpha=0.01, hidden_layer_sizes=(50,), score=-414.575, total=  27.1s
[CV] activation=relu, alpha=0.01, hidden_layer_sizes=(75,) ...........
[CV]  activation=relu, alpha=0.01, hidden_layer_sizes=(75,), score=-569.724, total=  38.6s
[CV] activation=relu, alpha=0.1, hidden_layer_sizes=(50,) ............
[CV]  activation=relu, alpha=0.1, hidden_layer_sizes=(50,), score=-414.668, total=  40.4s
[CV] activation=relu, alpha=0.1, hidden_layer_sizes=(100,) ...........
[CV]  activation=relu, alpha=0.1, hidden_layer_sizes=(100,), score=-298.842, total=  36.3s
[CV] n_estimators=75 .................................................
[CV] .................. n_estimators=75, score=-430.210, total=97.5min
/home/luciasalamanca/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 32155.701029108837, tolerance: 1531.0561481088264
  model = cd_fast.enet_coordinate_descent(
[CV] alpha=0.1 .......................................................
[CV] ........................ alpha=0.1, score=-339.204, total=   1.0s
[CV] alpha=2.0 .......................................................
[CV] ........................ alpha=2.0, score=-343.115, total=   0.5s
[CV] activation=relu, alpha=0.001, hidden_layer_sizes=(75,) ..........
[CV]  activation=relu, alpha=0.001, hidden_layer_sizes=(75,), score=-304.487, total=  21.5s
[CV] activation=relu, alpha=0.001, hidden_layer_sizes=(100,) .........
[CV]  activation=relu, alpha=0.001, hidden_layer_sizes=(100,), score=-290.095, total=  35.3s
[CV] activation=relu, alpha=0.01, hidden_layer_sizes=(50,) ...........
[CV]  activation=relu, alpha=0.01, hidden_layer_sizes=(50,), score=-359.832, total=  22.9s
[CV] activation=relu, alpha=0.01, hidden_layer_sizes=(75,) ...........
[CV]  activation=relu, alpha=0.01, hidden_layer_sizes=(75,), score=-369.764, total=  23.2s
[CV] activation=relu, alpha=0.1, hidden_layer_sizes=(50,) ............
[CV]  activation=relu, alpha=0.1, hidden_layer_sizes=(50,), score=-218.017, total=  55.8s
[CV] activation=relu, alpha=0.1, hidden_layer_sizes=(100,) ...........
[CV]  activation=relu, alpha=0.1, hidden_layer_sizes=(100,), score=-239.634, total=  14.8s
[CV] n_estimators=50 .................................................
[CV] .................. n_estimators=50, score=-396.550, total=76.7min
[CV] n_estimators=75 .................................................
[CV] .................. n_estimators=75, score=-505.864, total=67.6min
[CV] alpha=0.1 .......................................................
[CV] ........................ alpha=0.1, score=-348.270, total= 6.4min
[CV] alpha=0.1 .......................................................
[CV] ........................ alpha=0.1, score=-410.813, total=   0.8s
[CV] alpha=1.0 .......................................................
[CV] ........................ alpha=1.0, score=-326.611, total=   0.8s
[CV] alpha=1.0 .......................................................
[CV] ........................ alpha=1.0, score=-335.596, total=   0.9s
[CV] activation=relu, alpha=0.001, hidden_layer_sizes=(50,) ..........
[CV]  activation=relu, alpha=0.001, hidden_layer_sizes=(50,), score=-407.690, total=  48.1s
[CV] activation=relu, alpha=0.01, hidden_layer_sizes=(50,) ...........
[CV]  activation=relu, alpha=0.01, hidden_layer_sizes=(50,), score=-216.698, total=  21.1s
[CV] activation=relu, alpha=0.01, hidden_layer_sizes=(75,) ...........
[CV]  activation=relu, alpha=0.01, hidden_layer_sizes=(75,), score=-287.433, total=  25.4s
[CV] activation=relu, alpha=0.01, hidden_layer_sizes=(100,) ..........
[CV]  activation=relu, alpha=0.01, hidden_layer_sizes=(100,), score=-378.235, total=  34.1s
[CV] activation=relu, alpha=0.1, hidden_layer_sizes=(75,) ............
[CV]  activation=relu, alpha=0.1, hidden_layer_sizes=(75,), score=-313.701, total= 1.0min
[CV] n_estimators=50 .................................................
[CV] .................. n_estimators=50, score=-552.335, total=76.4min
[CV] n_estimators=75 .................................................
[CV] .................. n_estimators=75, score=-289.298, total=68.2min
/home/luciasalamanca/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 44324.875861982815, tolerance: 1664.2313559161787
  model = cd_fast.enet_coordinate_descent(
[Parallel(n_jobs=-1)]: Done  12 out of  15 | elapsed: 156.5min remaining: 39.1min
[CV] alpha=1 .........................................................
[CV] ......................... alpha=1, score=-4140.473, total=   1.1s
[CV] activation=relu, alpha=0.001, hidden_layer_sizes=(50,) ..........
[CV]  activation=relu, alpha=0.001, hidden_layer_sizes=(50,), score=-310.406, total=  39.9s
[CV] activation=relu, alpha=0.001, hidden_layer_sizes=(100,) .........
[CV]  activation=relu, alpha=0.001, hidden_layer_sizes=(100,), score=-604.852, total=  28.1s
[CV] activation=relu, alpha=0.01, hidden_layer_sizes=(75,) ...........
[CV]  activation=relu, alpha=0.01, hidden_layer_sizes=(75,), score=-201.873, total=  30.7s
[CV] activation=relu, alpha=0.01, hidden_layer_sizes=(100,) ..........
[CV]  activation=relu, alpha=0.01, hidden_layer_sizes=(100,), score=-615.648, total=  28.8s
[CV] activation=relu, alpha=0.1, hidden_layer_sizes=(75,) ............
[CV]  activation=relu, alpha=0.1, hidden_layer_sizes=(75,), score=-341.174, total=  17.6s
[CV] activation=relu, alpha=0.1, hidden_layer_sizes=(75,) ............
[CV]  activation=relu, alpha=0.1, hidden_layer_sizes=(75,), score=-429.878, total=  48.9s
[CV] n_estimators=50 .................................................
[CV] .................. n_estimators=50, score=-282.236, total=78.0min
[CV] n_estimators=100 ................................................
[CV] ................. n_estimators=100, score=-454.705, total=78.4min
Resultados para Random Forest
================================================================================

	--> Param: {'n_estimators': 50}, mean_score: -451.23595884429267, std_score: 101.68327332306899
	--> Param: {'n_estimators': 75}, mean_score: -447.9929522123837, std_score: 86.40509736923316
	--> Param: {'n_estimators': 100}, mean_score: -427.4321173203906, std_score: 89.16186529675821

--> CV -- No PCA
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed: 161.4min finished
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
[Parallel(n_jobs=-1)]: Done  12 out of  20 | elapsed:    1.2s remaining:    0.8s
Resultados para Lasso
================================================================================

	--> Param: {'alpha': 0.1}, mean_score: -476.16204829103737, std_score: 31.847432724881674
	--> Param: {'alpha': 1}, mean_score: -480.11993947953704, std_score: 35.482757681283594
	--> Param: {'alpha': 1.0}, mean_score: -480.11993947953704, std_score: 35.482757681283594
	--> Param: {'alpha': 2.0}, mean_score: -489.19237063597245, std_score: 39.415944879429375

Fitting 5 folds for each of 4 candidates, totalling 20 fits
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    1.7s finished
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
[Parallel(n_jobs=-1)]: Done  12 out of  20 | elapsed:    0.1s remaining:    0.1s
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.2s finished
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
Resultados para Ridge
================================================================================

	--> Param: {'alpha': 0.1}, mean_score: -478.3665618742022, std_score: 136.55998453017645
	--> Param: {'alpha': 1}, mean_score: -478.36503468920756, std_score: 136.63780420915543
	--> Param: {'alpha': 1.0}, mean_score: -478.36503468920756, std_score: 136.63780420915543
	--> Param: {'alpha': 2.0}, mean_score: -478.40594085233863, std_score: 136.68068618061668

Fitting 5 folds for each of 9 candidates, totalling 45 fits
[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:  1.1min
[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:  2.9min finished
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
Resultados para MLP
================================================================================

	--> Param: {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (50,)}, mean_score: -348.84930044324284, std_score: 89.3937988312519
	--> Param: {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (75,)}, mean_score: -348.67881445247565, std_score: 84.94618245403461
	--> Param: {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (100,)}, mean_score: -353.8470950470336, std_score: 92.69538820449732
	--> Param: {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (50,)}, mean_score: -393.20610408394566, std_score: 96.56872017405699
	--> Param: {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (75,)}, mean_score: -368.54272511296193, std_score: 95.31755272463631
	--> Param: {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (100,)}, mean_score: -349.88434144095334, std_score: 83.24779334085453
	--> Param: {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (50,)}, mean_score: -356.60299475255596, std_score: 103.09617401524835
	--> Param: {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (75,)}, mean_score: -345.30080167051557, std_score: 87.23858131482213
	--> Param: {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (100,)}, mean_score: -353.57928132379845, std_score: 87.35932295809071

Fitting 5 folds for each of 3 candidates, totalling 15 fits
[Parallel(n_jobs=-1)]: Done   6 out of  15 | elapsed:  1.2min remaining:  1.8min
[Parallel(n_jobs=-1)]: Done  12 out of  15 | elapsed:  2.3min remaining:   34.0s
[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  2.3min finished
Resultados para Random Forest
================================================================================

	--> Param: {'n_estimators': 50}, mean_score: -296.0560648822887, std_score: 43.519336788476686
	--> Param: {'n_estimators': 75}, mean_score: -285.71034811407105, std_score: 36.044800637258575
	--> Param: {'n_estimators': 100}, mean_score: -280.9575216263441, std_score: 35.46779460138012

